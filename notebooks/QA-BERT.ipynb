{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cefe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel,BertForQuestionAnswering,BertForMaskedLM,BertModel\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Set the logging level to WARNING (or ERROR) to suppress the warning\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507cca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./pretrained')\n",
    "QA = BertForQuestionAnswering.from_pretrained(\"./QA_model\")\n",
    "typo_fix = T5ForConditionalGeneration.from_pretrained(\"./typo\")\n",
    "typo_tokenizer = T5Tokenizer.from_pretrained(\"./typo_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc42aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"one two three four\",return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d835250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cf4b73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[unused564]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "vector_representation = outputs.last_hidden_state[:, 0, :]\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(vector_representation.argmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e384383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86da474c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18104/1010499668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pooler_output\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(end[\"pooler_output\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21371986",
   "metadata": {},
   "source": [
    "typo_tokenizer.save_pretrained(\"./typo_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597915e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ths is an exmple sentence with typos\"\n",
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "mask_idx = input_ids[0].tolist().index(tokenizer.mask_token_id)\n",
    "input_ids[0, mask_idx] = tokenizer.mask_token_id\n",
    "outputs = typo_fix(input_ids)\n",
    "predictions = outputs[0][0, mask_idx].topk(5)\n",
    "input_ids[0, mask_idx] = predictions.indices[0].item()\n",
    "fixed_sent = tokenizer.decode(input_ids[0])\n",
    "print(fixed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec309863",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25524/2627013549.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtypo_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mgenerated_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtypo_fix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mgen_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# fixed_sentence = typo_correction(sentence)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gen_ids' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def typo_correction(input_sentence):\n",
    "\n",
    "    input_sequence = f\"fix typo: {input_sentence}\"\n",
    "    input_ids = typo_tokenizer(input_sequence, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate the corrected sequence\n",
    "    generated_ids = typo_fix.generate(input_ids)\n",
    "\n",
    "    # Decode the generated sequence\n",
    "    corrected_sentence = typo_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "sentence = \"Ths is an exmple sentence with typos\"\n",
    "input_ids = typo_tokenizer(sentence,return_tensors = \"pt\")[\"input_ids\"]\n",
    "generated_ids = typo_fix.generate(input_ids)\n",
    "gen_ids\n",
    "\n",
    "# fixed_sentence = typo_correction(sentence)\n",
    "# print(\"Original sentence:\", sentence)\n",
    "# print(\"Corrected sentence:\", fixed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd62b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "def typo_correction(input_sentence, mask_idx, model):\n",
    "    tokenizer = BertTokenizer.from_pretrained('./pretrained')\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
    "    input_ids[0, mask_idx] = tokenizer.mask_token_id\n",
    "    \n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs[0][0, mask_idx].topk(5)\n",
    "    input_ids[0, mask_idx] = predictions.indices[0].item()\n",
    "    \n",
    "    fixed_sent = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return fixed_sent\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Ths is an exmple sentence with typos\"\n",
    "mask_idx = 6  # Adjust based on the position of the typo\n",
    "fixed_sentence = typo_correction(sentence, mask_idx, typo_fix)\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"Corrected sentence:\", fixed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77092080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is panda\"\n",
    "answer =\"\"\"The study abroad program (https://linktr.ee/binusstudyabroad) is a program that allows you to go to partner institutions with a credit transfer arrangement. This program can be an alternative enrichment track for those who need to do a 3+1 or (2+1)+1 enrichment semester.\n",
    "\n",
    "The benefits of taking the program are limitless! It is a rewarding experience that allows you to step out of your comfort zone, immerse yourself in an international environment, experience new cultures, potentially pick up a new language, and see the world from a different perspective. You will create a network for a lifetime and have the key to open the door of global opportunities.\n",
    "\n",
    "There are several types of programs that you can consider as your path of international study experience. Please look at the comparison table below for the guidelines.\n",
    "\n",
    "Program Type\n",
    "\n",
    "Study Abroad\tStudy Abroad\n",
    "Fees-Paying\n",
    "\n",
    "Double Degree\n",
    "\n",
    "Major\tAll majors, all campuses\tAll majors, all campuses\tOnly for students majoring in:\n",
    "Computer Science (BINUS @ Greater Jakarta)\n",
    "Marketing Communication (BINUS @ Greater Jakarta)\n",
    "Mass Communication (BINUS @ Greater Jakarta)\n",
    "BINUS @ Senayan\n",
    "Duration\t1 – 2 semesters\t1 – 2 semesters\t2 – 4 semesters\n",
    "Credit Transfer\tYes\tYes\tYes\n",
    "Degree\tBINUS Degree\tBINUS Degree\tBINUS Degree + Host Degree\n",
    "Semester Departure\tMin. 3rd semester Max. 7th semester\tMin. 3rd semester Max. 7th semester\tMin. 5th semester;\n",
    "it depends on the Double Degree arrangement\n",
    "Quota\tLimited with competition\tLimited with direct placement\tUnlimited if it meets the host institution’s requirements\n",
    "FINANCIAL PART\n",
    "\n",
    "Cost Component\n",
    "\n",
    "Study Abroad\tStudy Abroad\n",
    "Fees-Paying\n",
    "\n",
    "Double Degree\n",
    "\n",
    "Tuition Fee Payment\tBINUS Tuition: Yes\n",
    "Host Tuition: Free\t\n",
    "Non-Senayan Student\n",
    "BINUS Tuition: Yes\n",
    "Host Tuition: Yes\n",
    "Senayan Student\n",
    "BINUS Tuition: (BP3 or fixed tuition only)\n",
    "Host Tuition: Yes\n",
    "BINUS Tuition: Yes\n",
    "(BP3 or fixed tuition only)\n",
    "Host Tuition: Yes\n",
    "Airfare, Visa, Insurance, Living Cost\tSelf-funded\n",
    "GENERAL REQUIREMENTS\n",
    "\n",
    "Semester Departure\tStudy Abroad\tStudy Abroad\n",
    "Fees-Paying\n",
    "\n",
    "Double Degree\n",
    "Academic\tGPA min 2.75\t\n",
    "Passed required BINUS courses\n",
    "GPA depends on the host institution’s requirement\n",
    "Language Proficiency\tIELTS Academic 6.0 or​\n",
    "TOEFL iBT 80 or​\n",
    "TOEFL ITP 550​\n",
    "\n",
    "\"\"\"\n",
    "inputs = tokenizer(question, answer, return_tensors=\"pt\")\n",
    "\n",
    "outputs = QA(**inputs)\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores)\n",
    "\n",
    "answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1255e138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28e0c676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b6b0cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2040, 2003, 8915, 2684, 1997, 1037, 1029,  102, 1038, 1005, 1055,\n",
       "         2269, 1005, 1055, 1039, 1998, 2014, 2388, 2003, 1037,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03eb23bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.tensor>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

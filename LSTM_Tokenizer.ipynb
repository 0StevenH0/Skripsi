{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t-arvio.anandi/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/t-arvio.anandi/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel,BertForQuestionAnswering,BertForMaskedLM,BertModel\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t-arvio.anandi/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p2 were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded X Shape: (91, 40, 768)\n",
      "Padded Y Shape: (91, 40)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the IndoBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "bert_model = TFBertModel.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "\n",
    "file_path = '/Users/t-arvio.anandi/Downloads/Train Label - Sheet1 (2).csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "x_data_list = df['x_data'].tolist()\n",
    "y_data_list = df['y_data'].tolist()\n",
    "\n",
    "# Tokenize and encode\n",
    "encoding = tokenizer(x_data_list, padding='max_length', truncation=True, return_tensors='tf', max_length=40)\n",
    "x_input_ids = encoding['input_ids']\n",
    "x_attention_mask = encoding['attention_mask']\n",
    "\n",
    "# Pass the token IDs through BERT to get embeddings\n",
    "x_embeddings = bert_model(x_input_ids, attention_mask=x_attention_mask)[0].numpy()  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "# hidden_size is 768 for BERT-base, but we need 50 features\n",
    "\n",
    "# Desired size for the last dimension\n",
    "target_size = 50\n",
    "\n",
    "# Pad or truncate the last dimension\n",
    "if x_embeddings.shape[2] < target_size:\n",
    "    # Pad the last dimension\n",
    "    padding = target_size - x_embeddings.shape[2]\n",
    "    x_embeddings_padded = np.pad(x_embeddings, ((0, 0), (0, 0), (0, padding)), 'constant')\n",
    "else:\n",
    "    # Truncate the last dimension\n",
    "    x_embeddings_padded = x_embeddings[:, :, :target_size]\n",
    "\n",
    "\n",
    "# Convert y_data_list to strings and handle NaNs\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return [0]  # Replace NaN with a default value\n",
    "    try:\n",
    "        return list(map(int, label.split()))\n",
    "    except:\n",
    "        return [0]  # Handle any other conversion issues\n",
    "\n",
    "y_sequences = [clean_label(label) for label in y_data_list]\n",
    "\n",
    "# Pad y_sequences\n",
    "y_sequences_padded = pad_sequences(y_sequences, maxlen=40, padding='post', truncating='post', value=0)  # Using -1 or another placeholder for padding\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Padded X Shape:\", x_embeddings.shape)\n",
    "print(\"Padded Y Shape:\", y_sequences_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">918,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_11             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m768\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m918,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m197,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_11             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m7\u001b[0m)          │           \u001b[38;5;34m903\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,116,551</span> (4.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,116,551\u001b[0m (4.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,116,551</span> (4.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,116,551\u001b[0m (4.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the input layer to match the shape of your data\n",
    "inputs = layers.Input(shape=(40, 768))  # sequence_length=40, embedding_dim=768\n",
    "\n",
    "# LSTM layer\n",
    "lstm_out = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(inputs)\n",
    "lstm_out = layers.LSTM(128, return_sequences=True)(lstm_out)  # Stacked LSTM\n",
    "\n",
    "# Dense layer with ReLU activation\n",
    "relu_out = layers.TimeDistributed(layers.Dense(64, activation='relu'))(lstm_out)\n",
    "\n",
    "# TimeDistributed Dense layer for classification\n",
    "outputs = layers.TimeDistributed(layers.Dense(7, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))(lstm_out)\n",
    "\n",
    "# Define the model\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "def weighted_sparse_categorical_crossentropy(class_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        # Convert class_weights to a TensorFlow constant\n",
    "        class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\n",
    "        \n",
    "        # Gather weights for each sample\n",
    "        sample_weights = tf.gather(class_weights_tensor, tf.squeeze(y_true))\n",
    "        \n",
    "        # Apply weights to the loss\n",
    "        weighted_loss = loss * sample_weights\n",
    "        \n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    return loss\n",
    "\n",
    "class_weights = np.array([0.000000001, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss=weighted_sparse_categorical_crossentropy(class_weights),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X_sequences_padded and y_sequences_padded already padded\n",
    "X_sequences_padded_tensor = tf.convert_to_tensor(x_embeddings, dtype=tf.float32)  # or tf.int32\n",
    "y_sequences_padded_tensor = tf.convert_to_tensor(y_sequences_padded, dtype=tf.int32)  # or tf.int32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 40, 768)\n"
     ]
    }
   ],
   "source": [
    "print(X_sequences_padded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tensors to NumPy arrays\n",
    "X_sequences_padded_numpy = X_sequences_padded_tensor.numpy()\n",
    "y_sequences_padded_numpy = y_sequences_padded_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 40, 768)\n"
     ]
    }
   ],
   "source": [
    "print(X_sequences_padded_numpy.shape)  # Should match the model's expected input shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'k_neighbors' parameter of SMOTE must be an int in the range [1, inf) or an object implementing 'kneighbors' and 'kneighbors_graph'. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Apply SMOTE\u001b[39;00m\n\u001b[1;32m     10\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, k_neighbors\u001b[38;5;241m=\u001b[39mk_neighbors)\n\u001b[0;32m---> 11\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/imblearn/base.py:207\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/imblearn/base.py:42\u001b[0m, in \u001b[0;36m_ParamsValidationMixin._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mThe expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03maccepted constraints.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameter_constraints\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'k_neighbors' parameter of SMOTE must be an int in the range [1, inf) or an object implementing 'kneighbors' and 'kneighbors_graph'. Got 0 instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have more data, split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sequences_padded_numpy, y_sequences_padded_numpy, test_size=0.2)\n",
    "\n",
    "# Flatten y_train if it's one-hot encoded\n",
    "y_train_flat = np.argmax(y_train, axis=1)  # Assuming y_train is one-hot encoded\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train_flat)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.16373360e+00  1.64614451e+00  1.49624541e-01 ...  9.47471857e-01\n",
      "   -7.79526770e-01 -2.38206834e-01]\n",
      "  [-1.09512307e-01  5.61420798e-01  1.10080171e+00 ...  1.58512592e+00\n",
      "   -6.93527937e-01  1.52172685e-01]\n",
      "  [ 1.72116518e+00  5.08581460e-01 -8.53071928e-01 ...  9.59150255e-01\n",
      "   -1.68603480e+00  1.36044395e+00]\n",
      "  ...\n",
      "  [ 3.99216563e-01  9.81472373e-01  6.81728661e-01 ...  3.21035218e+00\n",
      "   -1.60785401e+00  6.23640642e-02]\n",
      "  [ 4.99320835e-01  1.28240681e+00  1.13290274e+00 ...  2.64770746e+00\n",
      "   -1.42777896e+00  1.16318412e-01]\n",
      "  [ 3.19074899e-01  9.92004991e-01  1.34694839e+00 ...  1.90102756e+00\n",
      "   -1.50747275e+00  1.98303416e-01]]\n",
      "\n",
      " [[ 3.14343989e-01  7.47709513e-01  2.43126631e-01 ...  1.49743712e+00\n",
      "   -7.04169929e-01  2.21151859e-04]\n",
      "  [ 1.13802421e+00 -4.13762420e-01 -3.24892879e-01 ...  5.58837205e-02\n",
      "   -1.15946376e+00  7.70162165e-01]\n",
      "  [-1.92225158e-01 -1.45542324e+00 -2.14756966e-01 ...  2.35511351e+00\n",
      "   -4.63654995e-02  6.24685824e-01]\n",
      "  ...\n",
      "  [-5.51110208e-02  4.09949899e-01  1.18079877e+00 ...  3.31589699e+00\n",
      "   -2.62769938e-01  4.33749616e-01]\n",
      "  [ 4.84859236e-02  8.00562501e-01  1.42870080e+00 ...  2.83850956e+00\n",
      "   -5.95505573e-02  5.86162627e-01]\n",
      "  [-2.70742983e-01  4.78399843e-02  1.60268891e+00 ...  2.20647311e+00\n",
      "   -5.36087215e-01  8.99246156e-01]]\n",
      "\n",
      " [[ 6.27203763e-01  1.18166161e+00 -4.32010032e-02 ...  9.88295197e-01\n",
      "   -1.08200252e+00 -4.73031610e-01]\n",
      "  [ 1.13365304e+00 -1.31574079e-01 -5.00255525e-01 ... -1.12044692e+00\n",
      "   -1.54725099e+00  1.53580987e+00]\n",
      "  [ 9.30936992e-01 -5.92260599e-01  7.82146573e-01 ...  6.33917272e-01\n",
      "   -5.24487734e-01 -6.05733767e-02]\n",
      "  ...\n",
      "  [ 7.58308053e-01  5.32313406e-01  1.36967123e+00 ...  2.06135726e+00\n",
      "   -2.99448699e-01  1.87332615e-01]\n",
      "  [ 5.32621861e-01  2.88640726e-02  1.14583635e+00 ...  2.57089376e+00\n",
      "   -5.66512287e-01  1.81921870e-01]\n",
      "  [ 6.29275203e-01  7.74337053e-01  1.20895207e+00 ...  1.76900148e+00\n",
      "   -3.52050513e-02  1.82002664e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 6.05861485e-01  1.17823243e+00  3.84980679e-01 ...  1.80284643e+00\n",
      "   -1.18145752e+00  1.05595633e-01]\n",
      "  [ 1.36431670e+00 -4.59955096e-01  3.62814963e-02 ...  1.32808656e-01\n",
      "   -1.07480240e+00  1.08593726e+00]\n",
      "  [ 9.56085801e-01 -2.33685583e-01 -5.28769493e-01 ...  2.87399507e+00\n",
      "    3.09139669e-01  2.94437818e-02]\n",
      "  ...\n",
      "  [ 3.88013840e-01  5.99201560e-01  1.16207874e+00 ...  3.62272668e+00\n",
      "   -6.34722888e-01  4.51674968e-01]\n",
      "  [ 3.23254824e-01  1.04915953e+00  1.50235319e+00 ...  3.23329782e+00\n",
      "   -5.14963329e-01  5.77070355e-01]\n",
      "  [-1.11976765e-01  2.76637316e-01  1.74061906e+00 ...  2.68919659e+00\n",
      "   -1.01165867e+00  8.89773309e-01]]\n",
      "\n",
      " [[ 4.40529674e-01  1.71412146e+00 -1.01442881e-01 ...  1.41715050e+00\n",
      "   -1.04034781e+00 -2.84162045e-01]\n",
      "  [ 1.53242993e+00 -1.78890526e-01 -1.53080714e+00 ...  5.99296987e-01\n",
      "   -6.94796145e-01  9.25397277e-01]\n",
      "  [ 1.21903276e+00  6.25882030e-01 -1.19176698e+00 ...  1.36627829e+00\n",
      "   -4.93481964e-01  7.24803627e-01]\n",
      "  ...\n",
      "  [ 1.04681504e+00  1.09932387e+00  7.51703620e-01 ...  2.77546692e+00\n",
      "   -6.32935911e-02  4.61204916e-01]\n",
      "  [-1.02824613e-01 -1.39778465e-01  4.39475119e-01 ...  2.83279371e+00\n",
      "   -8.74535739e-02  1.11790287e+00]\n",
      "  [ 9.87459242e-01  6.20238662e-01  6.24550760e-01 ...  3.17584658e+00\n",
      "   -3.18272978e-01  5.51006734e-01]]\n",
      "\n",
      " [[-2.70842552e-01  1.27993965e+00  3.56535465e-01 ...  1.43697214e+00\n",
      "   -7.94271886e-01 -7.54050791e-01]\n",
      "  [-5.29161692e-02 -1.09435990e-03 -1.69619620e-01 ...  1.10456908e+00\n",
      "   -6.32750273e-01  4.43380356e-01]\n",
      "  [-2.45654678e+00  1.33153692e-01  1.02014565e+00 ...  1.72712660e+00\n",
      "   -6.76279292e-02 -2.86020458e-01]\n",
      "  ...\n",
      "  [ 3.69409174e-01 -2.00108290e-02  4.02812898e-01 ...  2.26078653e+00\n",
      "   -6.70664787e-01  9.81050253e-01]\n",
      "  [-6.66998029e-02 -6.17881775e-01  6.63519502e-01 ...  2.41520667e+00\n",
      "   -9.40276027e-01  1.19240618e+00]\n",
      "  [ 5.46393618e-02  2.23614216e-01  1.08718121e+00 ...  2.51094198e+00\n",
      "   -9.72046494e-01  8.85259032e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert predictions to class indices\n",
    "predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Flatten the labels and predictions if needed\n",
    "Y_test_flattened = y_test.flatten()\n",
    "predicted_labels_flattened = predicted_labels.flatten()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(Y_test_flattened, predicted_labels_flattened)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new sentence and max length\n",
    "new_sentence = \"durasi durasi durasi durasi durasi durasi durasi program durasi durasi\"\n",
    "# words = new_sentence.split()\n",
    "# max_len = len(words)\n",
    "max_len = 40\n",
    "\n",
    "# Tokenize and convert to IDs\n",
    "new_tokens = tokenizer.tokenize(new_sentence)\n",
    "new_input_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "new_input_ids_padded = pad_sequences([new_input_ids], maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert input IDs to tensor\n",
    "new_input_ids_tensor = tf.convert_to_tensor(new_input_ids_padded, dtype=tf.int32)\n",
    "\n",
    "# Create an attention mask using TensorFlow operations\n",
    "attention_mask_tensor = tf.cast(new_input_ids_tensor != 0, dtype=tf.float32)\n",
    "\n",
    "# Compute embeddings using BERT\n",
    "outputs = bert_model(new_input_ids_tensor, attention_mask=attention_mask_tensor, return_dict=True)\n",
    "new_input_embeddings = outputs.last_hidden_state.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.76173073  1.5870137  -0.85510683 ...  0.02373251  1.3382697\n",
      "   -1.2021867 ]\n",
      "  [-0.7474993   1.3678035  -0.83683765 ...  0.16939129  0.8250834\n",
      "   -0.6025503 ]\n",
      "  [-0.7360506   1.4114292  -0.8065479  ...  0.15967149  0.7553942\n",
      "   -0.5666687 ]\n",
      "  ...\n",
      "  [-0.5694482   1.6982384  -0.3923992  ...  0.9461408   1.4252127\n",
      "   -0.03565612]\n",
      "  [-0.5575044   1.6410947  -0.4768017  ...  0.92158675  1.3660648\n",
      "    0.07664187]\n",
      "  [-0.6312691   1.6563826  -0.375901   ...  0.8670048   1.3082098\n",
      "    0.02709645]]]\n"
     ]
    }
   ],
   "source": [
    "print(new_input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3019fbaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "Trimmed Predicted Labels: tf.Tensor([[1 6 6 4 4 4 4 4 4 4]], shape=(1, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "words = new_sentence.split()\n",
    "max_len = len(words)\n",
    "\n",
    "# Predict token labels for the input sentence\n",
    "predictions = model.predict(new_input_embeddings)\n",
    "\n",
    "# Convert probabilities to predicted class labels\n",
    "predicted_labels = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "# Trim the predictions to keep only the first 7 (original) tokens\n",
    "trimmed_predictions = predicted_labels[:, :max_len]\n",
    "\n",
    "# Print the trimmed predicted labels\n",
    "print(\"Trimmed Predicted Labels:\", trimmed_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3019fbaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       712\n",
      "           1       0.03      1.00      0.05         7\n",
      "           2       0.01      1.00      0.02         1\n",
      "           3       0.02      1.00      0.05         1\n",
      "           4       0.28      1.00      0.44        23\n",
      "           6       0.05      1.00      0.10        16\n",
      "\n",
      "    accuracy                           0.06       760\n",
      "   macro avg       0.07      0.83      0.11       760\n",
      "weighted avg       0.01      0.06      0.02       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t-arvio.anandi/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/t-arvio.anandi/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/t-arvio.anandi/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "# Flatten the true labels and predictions\n",
    "y_true_flat = y_test.flatten()\n",
    "y_pred_flat = y_pred_classes.flatten()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true_flat, y_pred_flat))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
